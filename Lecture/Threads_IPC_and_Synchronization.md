# Threads, IPC, and Synchronization
## Threads
- Processes are very expensive because they require *their own* resources - such as their own address spaces
    - Processes, by nature, are distinct from each other, so they are not necessarily *meant* and usually *cannot* share resources
- Threads are more relevant in programs where this *resource sharing is necessary*, typically in order to work for a single goal
- **Threads** are units of execution and scheduling, each with their own stack, program counter, and registers
    - Multiple threads can run in a process, sharing the same code and data space - since they access the same resources, they are cheaper to create a run
- The CPU is shared between multiple threads, and this can be implemented either via user-level threads (yielding) or with scheduled system threads (preemption)
## Threads vs. Processes
- Processes should be used:
    - When there are multiple, distinct programs needing to be run
    - When there are only a small handful of programs needing to be run (the creation/destruction of processes are minimal)
    - When running agents have distinct privileges
    - When it is necessary to prevent interference between executing interpreters
    - When it is necessary to firewall one process from the failures of the other - one process failing may not necessarily cause another process to fail
- Threads should be used:
    - When there are parallel activities *in a single program* (i.e. a web server)
    - When there is frequent creation and destruction of running agents
    - When all running agents can run with the same privileges 
    - When it is necessary to share resources
    - When it is necessary to exchange many messages/signals between running agents
    - When it is not necessary to protect running agents from each other 
- Tradeoffs:
    - If multiple processes are used, the application may run much more slowly and there may be difficulty in sharing resources
    - If multiple threads are used, the creation and management of them is up to the user (not the operating system) - resulting in more complexity, not only in thread management but also in ensuring that resources can be serialized and threads are protected from each other (if necessary)
## Thread State and Thread Stacks
- Each thread has its own registers, processor status word, program counter, and its own stack area
    - The maximum stack size is specified when a thread is created since if it were otherwise able to grow indefinitely it could collide with other threads' stacks
## User Level Threads vs. Kernel Threads
- Kernel threads are an abstraction provided by the kernel that still involve one address space being shared but the *scheduling of threads being done by the kernel*
    - This can allow for multiple threads to use multiple cores at once
- User level threads are independent from the kernel and are provided and maanged via a user-level library - these threads are scheduled *by the library, not by the kernel*
## Communications Between Processes
- Distinct processes may occasionally need to exchange information - the mechanisms for **inter-process communications** is provided by the operating system, since processes cannot directly interact with each other
    - IPC requires activity from *both* communicating processes (both must agree to communicate), and this communication is mediated at each step by the operating system in order to protect both processes while also ensuring correct behavior
- For *local processes*, the data in the memory space of the sender needs to sent to the memory space of the receiver
    - In one approach, the operating system can copy the data between the two memory spaces
        - This is conceptually simpler, but results in potentially high overhead
    - In another approach, the operating system could use virtual memory techniques to *switch ownership* of the memory from the sender to the receiver 
        - This is much cheaper than copying the data, but requires changing page tables 
        - Additionally, only one of the two processes can see the data at a time (so the sender must no longer need the data after it is gone)
- **Synchronous IPC** involves the writer being *blocked* until the message is sent/delivered/received and the reader being *blocked* until a new message is available
    - This is very easy to implement but is obviously slow due to frequent blocking
- **Asynchronous IPC** involves the write operation returning when the system accepts the message and *not when it is confirmed to be delivered/received* and the read operations returning promptly if there is no message available
    - This requires an auxiliary mechanism to learn of errors in regards to the delivery of writes and an auxiliary mechanism to learn of new messages received in regards to reads
- Typical IPC Operations:
    - Create/Destroy a persistent IPC channel
    - Write/Send/Put data into the channel
    - Read/Receive/Get data from the channel
    - Query content from the channel (how much data is in the channel)
- **Streams** involve a continuous "flow" of bytes - few or many bytes may be read or written at a time
    - The write and read buffer sizes are unrelated, and the stream is delimited via program-specific conventions (like newlines, commas, etc.)
- **Messages (datagrams)** are a sequence of distinct messages, each with its own length (subject to limits)
    - Each message is typically read/written as a unit, and message delivery is usually all-or-nothing
- **Flow control**, in the context of interprocess communication, ensures that a fast sender does not overwhelm a slow receiver - this is especially relevant when considering communication via byte streams
    - Interprocess communications are queued in system resources (such as memory) until the receiver asks for it, but since this buffer space is finite, it is possible for the sender to fill up the entire space before the reader is able to read anything
    - To deal with this, either the operating system can block the sender from sending more data or the operating system can flush old data
        - It is best to make use of some sort of feedback system, such as by lowering the priority level of a sender so that they send data less frequently to the buffer
- It is important to ensure *reliability* with interprocess communications
    - This involves, for example, the question of when to signal the sender that their interprocess communication is complete - whether when the message is queued locally, when the message has been added to the receiver's input queue, when the receiver has actually read the message, or when the receiver has explicitly acknowledged the message
    - Reliability is even more important when dealing with *networks* as packet losses are incredibly common - this incurs the question of whether to attempt retransmissions, or to try different routes/servers, and so forth
    - Another question of reliability involves whether or not to maintain state when there are server crashes or process kills
- **Pipes** are one form of interprocess communication where a byte stream flows in a pipeline fashion - these bytes are buffered in the operating system (in memory), which thus does not require temporary files
    - Pipes are all under control of a single user, so there are no questions of security or privacy 
    - When a sender is *done* using a pipe, they would send a *end of file* message
- **Sockets** are connections between addresses/ports - there is a sender and receiver port (which listens for data)
    - Sockets present various data options, such as reliable (TCP) or best effort message (UDP), streams, messages, remote procedure calls, etc.
    - Due to their versality, sockets are associated with complex flow control and error handling - retransmissions, timeouts, node failures, etc. 
    - Sockets are not all under the control of a single user, so they require maintaining security between communicating processes
- **Shared memory** is another form of interprocess communication. The operating system arranges for processes to share *read/write* memory segments, which are mapped into such processes' address spaces (they point to the same physical page)
    - Applications must provide their own control and procedures for sharing; the operating system simply sets up the shared memory and is not involved in the actual data transfer
        - Since there is no operating system involvement, this approach is very *fast*
    - This can be simple in some instances but complicated in other instances since there is no guarantees of synchronization or error handling due to there being no operating system involvement - all of this must be done by the cooperating processes 
    - This approach only works on local machines since memory is *local*
## Synchronization
- Parallelism is important to have in systems because it improves throughput (activities can work at the same time instead of blocking each other), modularity (complex tasks are separated), and robustness (one thread failing does not stop others)
    - Parallelism is relevant to emerging paradigms, such as client-serving computing 
- **Synchronization** refers to the process of ensuring parallel activities (processes, threads, etc.) happen in the *correct* order
    - A program that is sequential is inherently deterministic and therefore trivial to ensure is synchronized
    - Similarly, independent parallel programs are easy to ensure synchronization since they do not interact any way
    - The issue emerges with *cooperating parallel programs*, as results depend on the order of instruction execution, which is non-deterministic (due to scheduling order)
- **Race conditions** occur when there are two or more things running in parallel
    - Typically race conditions do not matter (such as when there is no interaction between processes/threads), but there are cases where race conditions do impact *correctness*
        - Correctness-affecting race conditions may involve conflicting updates, check/act races (one process waiting for something to happen by another process), multi-object races (all-or-none transactions), and distributed computing decisions (which may be based on inconsistent views)
- **Non-deterministic execution** is execution that is not inherently predictable, often a result of activities such as I/O calls, time slicing, interrupts, unsynchronized interruption, etc.
### Critical Sections
- A **critical section** is a resource that is shared by multiple interpreters (threads, processes, CPUs, etc.) that can change in state when used 
    - Correctness for a critical section depends on *execution order*, which is heavily dependent on scheduling order (context switches)
        - While it is *unlikely* for a certain context switch to mess with the execution order, billions of instructions are executed per second so these unlikely events can happen quite frequently
- Issues concerning critical sections are addressed through **mutual exclusion**, which ensures that only *one thread can execute a critical section* at a time 
    - One way to get mutual exclusion is by *disabling interrupts* so that there is no preemption
        - This requires performing a privileged instruction, which is not feasible for user programs as they can potentially abuse the lack of interrupts (or be susceptible to a infinite loop bug which stops the entire machine)
            - Usually only the operating system itself does this in its kernel code, since it can be trusted
        - This also does not work with multi-core machines
    - Another, more common way, is to use *atomic instructions* to implement locks
        - **Atomic instructions** are uninterruptible read/modify/write operations, which can be used to implement locks that hold down critical sections